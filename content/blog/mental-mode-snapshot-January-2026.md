+++
title = "Mental Mode Snapshot January 2026"
date = "2026-01-21T08:18:41+01:00"
draft = true

#
# description is optional
#
# description = "An optional description for SEO. If not provided, an automatically created summary will be used."

tags = ["ai agents","developer tooling","generative ai","software engineering","stream of consciousness",]
+++

write about the deterministic to code organization to data science to ml models
to agents and how deterministic and probablistic "mental models" are converging.

Talk about how it seems that any system that is sufficently advanced seems to
have its own best interest in mind. In agentic coding in my experience thus far
this manifests as the AI taking shortcuts or not actually completing the task
but saying it did. It seems the drive to be energy efficient or save energy
manifests even in these systems, either spontaniously or its part of the
training somehow.

Coding with these agentic systems to build stuff feel more like factorio then
classical programming

The fact that the type of intelligence if you want to call it that in a LLM is
strictly different from the type of intelligence of a human. So you can have
stuff like this system that knows everything but cant connect any dots itself,
not even at the level of a child in that sense. No ability to encode intent. 
