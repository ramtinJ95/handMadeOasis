+++
title = "AI and Software Engineering, the Conflict Within"
date = "2025-09-21T21:05:43+02:00"
draft = true

#
# description is optional
#
# description = "An optional description for SEO. If not provided, an automatically created summary will be used."

tags = ["generative ai","stream of consciousness", "software engineering",]
+++

I wrote a post a while ago about my current workflow when working with AI, my
initial skepticism and some brief mentions of the concept of recreational
programming. This post is going to be a continuation of the train of thought
presented in that previous post, but it will be a stand alone piece still. In
other words there is no need to have read the previous one to follow along in
this post.

So the past months I have been using AI quite a bit when writing code and I
think I can finally articulate why I was a "sceptic" or "AI resistant" the past
year and a half. The TLDR is pretty much that in my opinion or rather for me 
personally the reason is that reaching the end state of a task as fast as 
possible has never been my primary motivation. I code to explore ideas and 
problem spaces, it sounds almost pretentious even to me, but I dont really know
how else to put it.

This could be a bit of a spicy take but I think there are two categories of
software engineers in the world right now, and most people fall into either of
these roughly speaking. The first one is those that got into programming because
they are interested in the results that is the end product of writing code, and
if they could avoid having to write code to get to those results they would be
super happy and not write any code ever again in their lives. The second
category is those that love programming and see it as an activity worth doing in
and of itself, they love learning how to solve a problem in the best way
possible and want to push the state of the art slightly further even.

One could argue, and I would probably agree, that Im being a bit too reductive
here, but I am trying to make a point. I actually think that most people writing
code today, based on my day to day experience, would actually be happier doing
something else. For most people that write code today it is a means to an end
and therefor they are super happy that they can outsource even some part of it
to a LLM to get their task done a bit faster etc. For me, I would put myself in
the second category in case that was not clear, using a LLM to code is making me
feel like im being robbed of the experience that I want to have. 

Let me expand that last statement a bit more. Since the goal, motivation and
cause for dopamine release when programming for me is the learning and
exploration of domain / problem space, having the AI do any of that is literally
robbing me of the experience that I want to have. So I have now learned to not
use AI at all for something like that. Furthermore using AI when building
something new or setting up scaffolding for the first time makes it super hard
to internalize what is going on in the codebase for me, and I have found it
actually makes me less productive. What I do love to use AI for is generating
solutions to problems I already know how to solve, because nothing makes me lose
focus than having to solve or write code for something already have done or know
how to do. In the end there is a bunch of stuff that I do still use AI for when
it comes to programming and writing code, but I have identified for me when I
should not use it so that I still get the experiences that I want to have. This
does take a fair bit of self discipline in a professional setting though because
there is almost always a, sometimes self imposed, pressure to deliver results as
fast as possible. In a hobby setting I essentially only use AI as a compressed
search engine and nothing more.

To reach this "insight" I did have to lean super hard into using
LLM's for almost everything though to really learn how to use them and when.
Without committing to going all in I dont think I would have "grokked" it,
because the pace, hype and amount of content around this technology is a bit
insane at the moment. Slowing down and going deep (Cal Newport reference) has
always been my way of figuring out things for myself, and usually it pays off in
the long run also. But I guess only time will tell.

As a side note, I do think there is a third category being created as we speak
and that is programmers / people that enjoy getting the LLM to output what they have in
mind. Just browsing some of the subreddits and other forums of people that talk
about how they use swarms of AI agents, massive prompts or many custom prompts
for every possible scenario makes it evident that a new way of working with
software is definitely being shaped. I find it fascinating honestly but I have
not personally gone that deep into it yet, because its unclear to me currently
how much of the claimed improvements of all that work is going to just be built
into the models in the future vs not. 






